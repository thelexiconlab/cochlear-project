{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Getting the full similarities dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import nltk\n",
    "import scipy\n",
    "from functools import lru_cache\n",
    "from itertools import product as iterprod\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class phonology_funcs:\n",
    "    '''\n",
    "        Description: \n",
    "            This class contains functions to generate phonemes from a list of words and create a phonological similarity matrix.\n",
    "            Code has been adapted from the following link: https://stackoverflow.com/questions/33666557/get-phonemes-from-any-word-in-python-nltk-or-other-modules\n",
    "        Functions:\n",
    "            (1) load_arpabet(): loads and returns the arpabet dictionary from the NLTK CMU dictionary\n",
    "            (2) wordbreak(s, arpabet): takes in a word (str) and an arpabet dictionary and returns a list of phonemes\n",
    "            (3) normalized_edit_distance(w1, w2): takes in two strings (w1, w2) and returns the normalized edit distance between them\n",
    "            (3) create_phonological_matrix: takes in a list of labels (size N) and returns a phonological similarity matrix (NxN np.array)\n",
    "    '''\n",
    "    @lru_cache()\n",
    "    def wordbreak(s):\n",
    "        '''\n",
    "            Description:\n",
    "                Takes in a word (str) and an arpabet dictionary and returns a list of phonemes\n",
    "            Args:\n",
    "                (1) s (str): string to be broken into phonemes\n",
    "            Returns:\n",
    "                (1) phonemes (list, size: variable): list of phonemes in s \n",
    "        '''\n",
    "        try:\n",
    "            arpabet = nltk.corpus.cmudict.dict()\n",
    "        except LookupError:\n",
    "            nltk.download('cmudict')\n",
    "            arpabet = nltk.corpus.cmudict.dict()\n",
    "                \n",
    "        s = s.lower()\n",
    "        if s in arpabet:\n",
    "            return arpabet[s]\n",
    "        middle = len(s)/2\n",
    "        partition = sorted(list(range(len(s))), key=lambda x: (x-middle)**2-x)\n",
    "        for i in partition:\n",
    "            pre, suf = (s[:i], s[i:])\n",
    "            if pre in arpabet and phonology_funcs.wordbreak(suf) is not None:\n",
    "                return [x+y for x,y in iterprod(arpabet[pre], phonology_funcs.wordbreak(suf))]\n",
    "        return None\n",
    "\n",
    "    def normalized_edit_distance(w1, w2):\n",
    "        '''\n",
    "            Description: \n",
    "                Takes in two strings (w1, w2) and returns the normalized edit distance between them\n",
    "            Args:\n",
    "                (1) w1 (str): first word\n",
    "                (2) w2 (str): second word\n",
    "            Returns:\n",
    "                (1) normalized_edit_distance (float): normalized edit distance between w1 and w2\n",
    "        '''\n",
    "        return round(1-nltk.edit_distance(w1,w2)/(max(len(w1), len(w2))),4)\n",
    "\n",
    "    def phonological_similarity(word1, word2): \n",
    "        phon_sim = phonology_funcs.normalized_edit_distance(phonology_funcs.wordbreak(word1)[0], phonology_funcs.wordbreak(word2)[0])\n",
    "        return phon_sim\n",
    "\n",
    "\n",
    "def semantic_similarity(word1, word2, path_to_embeddings): \n",
    "    embeddings = pd.read_csv(path_to_embeddings)\n",
    "    \n",
    "    word1_embedding = embeddings[word1].T\n",
    "    word2_embedding = embeddings[word2].T\n",
    "    \n",
    "    similarity = 1 - scipy.spatial.distance.cosine(word1_embedding, word2_embedding)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_similarity_table(main_word, word_list, path_to_embeddings, path_save_file): \n",
    "    df = pd.DataFrame()\n",
    "    semantic_similarities = []\n",
    "    phonological_similarities = []\n",
    "    for word in word_list: \n",
    "        semantic_similarities.append(semantic_similarity(main_word, word, path_to_embeddings))\n",
    "        phonological_similarities.append(phonology_funcs.phonological_similarity(main_word, word))\n",
    "    df['Word'] = word_list\n",
    "    df['semantic_similarity'] = semantic_similarities\n",
    "    df['phonological_similarity'] = phonological_similarities\n",
    "    df.to_csv(path_save_file, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = pd.read_csv(\"../forager/data/lexical_data/50_dim_lexical_data/alpha_0.0_s2v/frequencies.csv\",header=None)[0].values.tolist()\n",
    "\n",
    "\n",
    "\n",
    "create_similarity_table(\"pets\", word_list, \"../forager/data/lexical_data/100_dim_lexical_data/alpha_0.3_w2v/embeddings.csv\", \"../forager/output/CBM with pets/CBM_100_dim_alpha_0.3_w2v_full_sim.csv\")\n",
    "create_similarity_table(\"pets\", word_list, \"../forager/data/lexical_data/100_dim_lexical_data/alpha_0.2_w2v/embeddings.csv\", \"../forager/output/CBM with pets/CBM_100_dim_alpha_0.2_w2v_full_sim.csv\")\n",
    "create_similarity_table(\"pets\", word_list, \"../forager/data/lexical_data/100_dim_lexical_data/alpha_0.5_w2v/embeddings.csv\", \"../forager/output/CBM with pets/CBM_100_dim_alpha_0.5_w2v_full_sim.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. checking if sim history and phon history are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_history_variables(fluency_list, labels, sim_matrix, freq_matrix, phon_matrix = None):\n",
    "    '''\n",
    "        Args:\n",
    "            (1) sim_matrix: semantic similarity matrix (NxN np.array)\n",
    "            (2) phon_matrix: phonological similarity matrix (NxN np.array)\n",
    "            (3) freq_matrix: frequencies array (Nx1 array)\n",
    "            (4) labels: the space of words (list of length N)\n",
    "            (5) fluency_list: items produced by a participant (list of size L)\n",
    "\n",
    "        Returns: \n",
    "            (1) sim_list (list, size: L): semantic similarities between each item in fluency_list \n",
    "            (2) sim_history(list, size: L arrays of size N): semantic similarities of each word in fluency_list with all items in labels\n",
    "            (3) phon_list (list, size: L): phonological similarities between each item in fluency_list \n",
    "            (4) phon_history (list, size: L arrays of size N): phonological similarities of each word in fluency_list with all items in labels\n",
    "            (5) freq_list (list, size: L): frequencies of each item in fluency_list (list of size L)\n",
    "            (6) freq_history  (list, size: L arrays of size N): frequencies of all words in labels repeated L times\n",
    "\n",
    "\n",
    "    '''\n",
    "    if phon_matrix is not None:\n",
    "        phon_matrix[phon_matrix <= 0] = .0001\n",
    "    sim_matrix[sim_matrix <= 0] = .0001\n",
    "\n",
    "    freq_list = []\n",
    "    freq_history = []\n",
    "\n",
    "    sim_list = []\n",
    "    sim_history = []\n",
    "\n",
    "    phon_list = []\n",
    "    phon_history = []\n",
    "\n",
    "    for i in range(0,len(fluency_list)):\n",
    "        word = fluency_list[i]\n",
    "        currentwordindex = labels.index(word)\n",
    "\n",
    "        freq_list.append(freq_matrix[currentwordindex])\n",
    "        freq_history.append(freq_matrix)\n",
    "\n",
    "        if i > 0: # get similarity between this word and preceding word\n",
    "            prevwordindex = labels.index(fluency_list[i-1])\n",
    "            sim_list.append(sim_matrix[prevwordindex, currentwordindex] )\n",
    "            sim_history.append(sim_matrix[prevwordindex,:])\n",
    "            if phon_matrix is not None:\n",
    "                phon_list.append(phon_matrix[prevwordindex, currentwordindex] )\n",
    "                phon_history.append(phon_matrix[prevwordindex,:])\n",
    "        else: # first word\n",
    "            sim_list.append(0.0001)\n",
    "            sim_history.append(sim_matrix[currentwordindex,:])\n",
    "            if phon_matrix is not None:\n",
    "                phon_list.append(0.0001)\n",
    "                phon_history.append(phon_matrix[currentwordindex,:])\n",
    "\n",
    "    return sim_list, sim_history, freq_list, freq_history,phon_list, phon_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = '100'\n",
    "type = 'alpha_0.3_w2v'\n",
    "\n",
    "similaritypath =  '../forager/data/lexical_data/' + dimension + '_dim_lexical_data/' + type + '/semantic_matrix.csv'\n",
    "frequencypath =  '../forager/data/lexical_data/' + dimension + '_dim_lexical_data/' + type + '/frequencies.csv'\n",
    "phonpath = '../forager/data/lexical_data/' + dimension + '_dim_lexical_data/' + type + '/phonological_matrix.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "CBM_words = pd.read_csv(\"../forager/data/fluency_lists/participant_data/individual participants/CBM_original.txt\", delimiter=\"\\t\")\n",
    "CBM_words = CBM_words[\"Word\"].tolist() \n",
    "print(CBM_words.index(\"pets\"))\n",
    "print(len(CBM_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = np.loadtxt(similaritypath,delimiter=',')\n",
    "frequency_list = np.array(pd.read_csv(frequencypath,header=None,encoding=\"unicode-escape\")[1])\n",
    "phon_matrix = np.loadtxt(phonpath,delimiter=',')\n",
    "labels = pd.read_csv(frequencypath,header=None)[0].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_list, sim_history, freq_list, freq_history,phon_list, phon_history = create_history_variables(CBM_words, labels, similarity_matrix, frequency_list, phon_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.42200345 0.3977776  0.54317008 0.32422199 0.34682831 0.37701656\n",
      " 0.49864981 0.47863588 0.38275885 0.24945657 0.47634934 0.45838574\n",
      " 0.49009323 0.43078561 0.31440982 0.30736378 0.38087381 0.38051933\n",
      " 0.47590154 0.34616522 0.36365902 0.4548852  0.6049997  0.52614617\n",
      " 0.3974489  0.39782991 0.43889562 0.53719206 0.44047277 0.43518821\n",
      " 0.43629137 0.44907354 0.39160627 0.23604724 0.31933634 0.42299988\n",
      " 0.52672368 0.45149767 0.40753074 0.34309711 0.35732356 0.39918935\n",
      " 0.34744203 0.38301354 0.37173505 0.48718404 0.45050806 0.31540856\n",
      " 0.29797146 0.5228793  0.49793375 0.2868362  0.34567857 0.43278755\n",
      " 0.40596105 1.         0.39615038 0.25851114 0.36664081 0.39790752\n",
      " 0.30991125 0.38098716 0.41059097 0.53358238 0.443584   0.38997211\n",
      " 0.35236891 0.31587222 0.45666435 0.30808408 0.41514922 0.41023338\n",
      " 0.57760077 0.37376474 0.36500429 0.39370847 0.44849987 0.33231089\n",
      " 0.18573687 0.30043806 0.34675023 0.36640524 0.39215048 0.33124591\n",
      " 0.47818152 0.46526002 0.4044614  0.41318129 0.48784176 0.39417342\n",
      " 0.55630474 0.47239386 0.48381681 0.54161396 0.40298239 0.47245609\n",
      " 0.21755028 0.44051582 0.28882502 0.31973542 0.42721926 0.47638377\n",
      " 0.6955222  0.52629706 0.42843918 0.2975731  0.4214073  0.56228998\n",
      " 0.36641422 0.54429598 0.33751139 0.20755923 0.39960782 0.46394433\n",
      " 0.4947641  0.48729834 0.47338628 0.48170123 0.45775345 0.477142\n",
      " 0.414468   0.39526808 0.37907165 0.42893825 0.54870031 0.47559808\n",
      " 0.50249242 0.50028087 0.34530402 0.41609211 0.32675529 0.44884823\n",
      " 0.35159662 0.52159618 0.36667714 0.28819537 0.33386953 0.49049267\n",
      " 0.30946699 0.49993717 0.52433726 0.45521422 0.35373134 0.46549429\n",
      " 0.28588409 0.30483329 0.44076057 0.36452149 0.26386481 0.33782077\n",
      " 0.47350091 0.27859753 0.15624546 0.1281153  0.45863694 0.52782003\n",
      " 0.38616777 0.44557663 0.5033645  0.42826675 0.51640771 0.24504793\n",
      " 0.39054696 0.50070626 0.29628931 0.45273428 0.46478669 0.44054638\n",
      " 0.45366511 0.37634413 0.32868877 0.42789922 0.29934786 0.3990455\n",
      " 0.19420571 0.34746521 0.30711104 0.52072854 0.27492213 0.50193148\n",
      " 0.49807478 0.47265872 0.60224221 0.38266758 0.4080059  0.3739565\n",
      " 0.4361689  0.51343043 0.40171021 0.37424918 0.31602778 0.44532952\n",
      " 0.29249295 0.45075405 0.28870587 0.28996933 0.44034029 0.45993084\n",
      " 0.41972199 0.48502608 0.45449725 0.42430736 0.42004202 0.42136014\n",
      " 0.4903631  0.40968904 0.3741174  0.51324949 0.38608159 0.42179468\n",
      " 0.47394988 0.43442638 0.56476631 0.34953153 0.31113429 0.16657955\n",
      " 0.43479544 0.40185165 0.39689522 0.36667596 0.5245097  0.33491349\n",
      " 0.42832224 0.48222503 0.48856992 0.51386419 0.52073522 0.42224499\n",
      " 0.51227138 0.45408251 0.36861227 0.28015987 0.4089989  0.35873598\n",
      " 0.39989588 0.43662768 0.53730554 0.27979761 0.52227588 0.44660406\n",
      " 0.43559734 0.3714107  0.39090433 0.34220402 0.4073549  0.53273634\n",
      " 0.4993226  0.37821014 0.31167623 0.17416314 0.5594889  0.46721584\n",
      " 0.39060363 0.51973772 0.41852858 0.29760997 0.42018185 0.27782488\n",
      " 0.42643582 0.33682879 0.50715324 0.22505883 0.57698873 0.45532536\n",
      " 0.41982071 0.49923507 0.2159757  0.40572413 0.40418278 0.31460528\n",
      " 0.57790233 0.19624476 0.23842797 0.42344136 0.45275464 0.40050477\n",
      " 0.3555685  0.1951181  0.26128993 0.36281496 0.4530934  0.37096118\n",
      " 0.45540638 0.44607032 0.42030449 0.39852065 0.52545186 0.40925419\n",
      " 0.29878972 0.34569751 0.3599482  0.45836212 0.17463064 0.34308428\n",
      " 0.50242635 0.3726483  0.41921785 0.4533182  0.36279654 0.44514233\n",
      " 0.24485675 0.35208426 0.49095399 0.55089239 0.23961772 0.51578616\n",
      " 0.52465539 0.35853256 0.24239523 0.36772814 0.22426964 0.45693801\n",
      " 0.3059148  0.56503299 0.34566265 0.36995304 0.34502846 0.40095799\n",
      " 0.43270625 0.16066161 0.48638262 0.31282142 0.56411475 0.43382559\n",
      " 0.499974   0.55513481 0.56317811 0.51159325 0.41142456 0.57335488\n",
      " 0.3826011  0.57424709 0.41105845 0.4551794  0.42179854 0.1673617\n",
      " 0.27818522 0.42375766 0.57105501 0.422811   0.50718341 0.45136345\n",
      " 0.35821657 0.26396544 0.42931311 0.38429279 0.41244445 0.1855027\n",
      " 0.36316295 0.46087389 0.26093787 0.43454958 0.30723428 0.47795776\n",
      " 0.3074883  0.41812556 0.32515591 0.35193783 0.30264348 0.34012033\n",
      " 0.44348398 0.5046044  0.38603753 0.43389962 0.41137871 0.22578965\n",
      " 0.30955297 0.26005654 0.46948989 0.49851053 0.42164773 0.47874158\n",
      " 0.35874828 0.55946423 0.4868806  0.33961306 0.19482817 0.3864918\n",
      " 0.46933116 0.55309891 0.4699833  0.42584425 0.49788052 0.42836592\n",
      " 0.25432049 0.23378788 0.38299231 0.34230212 0.36111283 0.53039376\n",
      " 0.35021051 0.19861321 0.40286226 0.64276435 0.48167608 0.27079866\n",
      " 0.44250934 0.36309879 0.52908926 0.43320746 0.48949771 0.2950735\n",
      " 0.59976153 0.57771027 0.43140799 0.48087689 0.12468815 0.37279335\n",
      " 0.27618088 0.43889025 0.25546452 0.36557908 0.35837743 0.4050724\n",
      " 0.48143277 0.32043644 0.54458192 0.3435669  0.45021901 0.45932457\n",
      " 0.38295801 0.47208854 0.41624275 0.43570192 0.41227035 0.57880703\n",
      " 0.37757508 0.32293187 0.30950187 0.40365053 0.38453819 0.52407325\n",
      " 0.26054495 0.49587614 0.33691876 0.4288704  0.46549325 0.37841618\n",
      " 0.39155885 0.44747211 0.53560807 0.41403938 0.39873121 0.47551156\n",
      " 0.49264199 0.49628814 0.26840687 0.1726409  0.2893573  0.40855921\n",
      " 0.45212697 0.44639925 0.30316394 0.45020237 0.24969738 0.25175932\n",
      " 0.18349217 0.23592793 0.34340367 0.25883498 0.34177793 0.36214308\n",
      " 0.4712811 ]\n"
     ]
    }
   ],
   "source": [
    "print(sim_history[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 463)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(sim_history).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Creating CSV File for error_results.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n"
     ]
    }
   ],
   "source": [
    "errors = ['../forager/output/Test Error/With Rep With Sub/', \n",
    "          '../forager/output/Test Error/With Rep Without Sub/', \n",
    "          '../forager/output/Test Error/Without Rep With Sub/', \n",
    "          '../forager/output/Test Error/Without Rep Without Sub/']\n",
    "\n",
    "rep_sub = [('with_rep', 'with_sub'), ('with_rep', 'without_sub'), ('without_rep', 'with_sub'), ('without_rep', 'without_sub')]\n",
    "\n",
    "\n",
    "# data_path = ['../forager/data/fluency_lists/participant_data/Error Testing/error_with_rep_with_sub.txt', \n",
    "#              '../forager/data/fluency_lists/participant_data/Error Testing/error_with_rep_without_sub.txt', \n",
    "#              '../forager/data/fluency_lists/participant_data/Error Testing/error_without_rep_with_sub.txt', \n",
    "#              '../forager/data/fluency_lists/participant_data/Error Testing/error_without_rep_without_sub.txt' ]\n",
    "\n",
    "dimensions = ['50', '100', '200', '300']\n",
    "type = [\n",
    "    'alpha_0.0_s2v', # = alpha_1_w2v\n",
    "    'alpha_0.0_w2v', # = alpha_1_s2v \n",
    "    'alpha_0.1_s2v', # = alpha_0.9_w2v\n",
    "    'alpha_0.1_w2v', # = alpha_0.9_s2v\n",
    "    'alpha_0.2_s2v', # = alpha_0.8_w2v\n",
    "    'alpha_0.2_w2v', # = alpha_0.8_s2v\n",
    "    'alpha_0.3_s2v', # = alpha_0.7_w2v\n",
    "    'alpha_0.3_w2v', # = alpha_0.7_s2v \n",
    "    'alpha_0.4_s2v', # = alpha_0.6_w2v\n",
    "    'alpha_0.4_w2v', # = alpha_0.6_s2v\n",
    "    'alpha_0.5_s2v', # = alpha_0.5_w2v\n",
    "    'average',\n",
    "    'only_w2v', \n",
    "    'only_s2v'\n",
    "]\n",
    "\n",
    "# forager results \n",
    "forager_paths = ['../forager/output/Test Error/Forager Errors/error_with_rep_with_sub_forager_results/model_results.csv', \n",
    "         '../forager/output/Test Error/Forager Errors/error_with_rep_without_sub_forager_results/model_results.csv',\n",
    "         '../forager/output/Test Error/Forager Errors/error_without_rep_with_sub_forager_results/model_results.csv',\n",
    "         '../forager/output/Test Error/Forager Errors/error_without_rep_without_sub_forager_results/model_results.csv'\n",
    "         ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "error_results = pd.DataFrame(columns=[\"Project\", \"Repetition\", \"Substitution\", \"Dimension\", \"Alpha\", \"Missing NLL\"])\n",
    "\n",
    "for i in range(4): \n",
    "    df = pd.read_csv(forager_paths[i])\n",
    "    nan_NLL = df['Negative_Log_Likelihood_Optimized'].isnull().any()\n",
    "    error_results.loc[len(error_results)] = [\"Forager\", rep_sub[i][0], rep_sub[i][1], \"N/A\", \"N/A\", nan_NLL]\n",
    "\n",
    "for i in range(4): \n",
    "    for dim in dimensions: \n",
    "        for t in type: \n",
    "            path = errors[i] + dim + \"_dim_results/\" + t + \"_results\"+ \"/model_results.csv\"\n",
    "            df = pd.read_csv(path)\n",
    "            nan_NLL = df[\"Negative_Log_Likelihood_Optimized\"].isnull().any()\n",
    "            error_results.loc[len(error_results)] = [\"Cochlear Project\", rep_sub[i][0], rep_sub[i][1], dim, t, nan_NLL]\n",
    "\n",
    "\n",
    "\n",
    "error_results.to_csv(\"../error analysis/error_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Project</th>\n",
       "      <th>Repetition</th>\n",
       "      <th>Substitution</th>\n",
       "      <th>Dimension</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Missing NLL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Forager</td>\n",
       "      <td>with_rep</td>\n",
       "      <td>with_sub</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forager</td>\n",
       "      <td>with_rep</td>\n",
       "      <td>without_sub</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Forager</td>\n",
       "      <td>without_rep</td>\n",
       "      <td>with_sub</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Forager</td>\n",
       "      <td>without_rep</td>\n",
       "      <td>without_sub</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cochlear Project</td>\n",
       "      <td>with_rep</td>\n",
       "      <td>with_sub</td>\n",
       "      <td>50</td>\n",
       "      <td>alpha_0.0_s2v</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>Cochlear Project</td>\n",
       "      <td>without_rep</td>\n",
       "      <td>without_sub</td>\n",
       "      <td>300</td>\n",
       "      <td>alpha_0.4_w2v</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>Cochlear Project</td>\n",
       "      <td>without_rep</td>\n",
       "      <td>without_sub</td>\n",
       "      <td>300</td>\n",
       "      <td>alpha_0.5_s2v</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>Cochlear Project</td>\n",
       "      <td>without_rep</td>\n",
       "      <td>without_sub</td>\n",
       "      <td>300</td>\n",
       "      <td>average</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>Cochlear Project</td>\n",
       "      <td>without_rep</td>\n",
       "      <td>without_sub</td>\n",
       "      <td>300</td>\n",
       "      <td>only_w2v</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>Cochlear Project</td>\n",
       "      <td>without_rep</td>\n",
       "      <td>without_sub</td>\n",
       "      <td>300</td>\n",
       "      <td>only_s2v</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>228 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Project   Repetition Substitution Dimension          Alpha  \\\n",
       "0             Forager     with_rep     with_sub       N/A            N/A   \n",
       "1             Forager     with_rep  without_sub       N/A            N/A   \n",
       "2             Forager  without_rep     with_sub       N/A            N/A   \n",
       "3             Forager  without_rep  without_sub       N/A            N/A   \n",
       "4    Cochlear Project     with_rep     with_sub        50  alpha_0.0_s2v   \n",
       "..                ...          ...          ...       ...            ...   \n",
       "223  Cochlear Project  without_rep  without_sub       300  alpha_0.4_w2v   \n",
       "224  Cochlear Project  without_rep  without_sub       300  alpha_0.5_s2v   \n",
       "225  Cochlear Project  without_rep  without_sub       300        average   \n",
       "226  Cochlear Project  without_rep  without_sub       300       only_w2v   \n",
       "227  Cochlear Project  without_rep  without_sub       300       only_s2v   \n",
       "\n",
       "     Missing NLL  \n",
       "0          False  \n",
       "1          False  \n",
       "2          False  \n",
       "3          False  \n",
       "4          False  \n",
       "..           ...  \n",
       "223        False  \n",
       "224        False  \n",
       "225        False  \n",
       "226        False  \n",
       "227        False  \n",
       "\n",
       "[228 rows x 6 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
