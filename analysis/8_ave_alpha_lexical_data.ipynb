{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the embeddings for the average and alpha values of speech2vec and word2vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os \n",
    "import shutil \n",
    "import matplotlib as mpl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [\"50\", \"100\", \"200\", \"300\"]\n",
    "\n",
    "lexical_data_path = \"../forager/data/lexical_data/\"\n",
    "s2v_path = \"_dim_lexical_data/only_s2v/embeddings.csv\"\n",
    "w2v_path = \"_dim_lexical_data/only_w2v/embeddings.csv\"\n",
    "folder = \"_dim_lexical_data/average\"\n",
    "embeddings = \"/embeddings.csv\"\n",
    "\n",
    "\n",
    "for dim in dimensions: \n",
    "    os.makedirs(lexical_data_path + dim + folder, exist_ok=True)\n",
    "    \n",
    "    s2v = pd.read_csv(lexical_data_path + dim + s2v_path)\n",
    "    w2v = pd.read_csv(lexical_data_path + dim + w2v_path)\n",
    "    df_mean = (s2v + w2v) / 2\n",
    "    \n",
    "    df_mean.to_csv(lexical_data_path + dim + folder + embeddings, index= False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpha embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n"
     ]
    }
   ],
   "source": [
    "dimensions = [\"50\", '100', '200', '300']\n",
    "alphas = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "models = [\"s2v\", \"w2v\"]\n",
    "\n",
    "\n",
    "# paths \n",
    "lexical_data_path = \"../forager/data/lexical_data/\"\n",
    "s2v_path = \"_dim_lexical_data/only_s2v/embeddings.csv\"\n",
    "w2v_path = \"_dim_lexical_data/only_w2v/embeddings.csv\"\n",
    "folder = \"_dim_lexical_data/alpha_\"\n",
    "embeddings = \"/embeddings.csv\"\n",
    "\n",
    "count = 0 \n",
    "for dim in dimensions: \n",
    "    for model in models: \n",
    "        for alp in alphas: \n",
    "            \n",
    "            if model == \"s2v\": \n",
    "                path = lexical_data_path + dim + folder + str(alp) + \"_\" + \"s2v\"\n",
    "                os.makedirs(path, exist_ok=True)\n",
    "\n",
    "                s2v = pd.read_csv(lexical_data_path + dim + s2v_path)\n",
    "                w2v = pd.read_csv(lexical_data_path + dim + w2v_path)\n",
    "                \n",
    "                s2v_alpha = s2v * alp \n",
    "                w2v_alpha = w2v * (1 - alp)\n",
    "                \n",
    "                df_combined = pd.concat([s2v_alpha, w2v_alpha], ignore_index=True)\n",
    "                df_combined.to_csv(path + '/embeddings.csv', index = False)\n",
    "                \n",
    "            if model == \"w2v\": \n",
    "                path = lexical_data_path + dim + folder + str(alp) + \"_\" + \"w2v\"\n",
    "                os.makedirs(path, exist_ok=True)\n",
    "\n",
    "                w2v = pd.read_csv(lexical_data_path + dim + w2v_path)\n",
    "                s2v = pd.read_csv(lexical_data_path + dim + s2v_path)\n",
    "                \n",
    "                w2v_alpha = w2v * alp \n",
    "                s2v_alpha = s2v * (1 - alp)\n",
    "                \n",
    "                df_combined = pd.concat([w2v_alpha, s2v_alpha], ignore_index=True)\n",
    "                df_combined.to_csv(path + \"/embeddings.csv\", index = False) \n",
    "                \n",
    "            count += 1 \n",
    "            \n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''All paths for embeddings'''\n",
    "\n",
    "alpha_path = [] \n",
    "\n",
    "ave_path = [] \n",
    "\n",
    "only_path = [] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha path num: 88\n",
      "['../forager/data/lexical_data/50_dim_lexical_data/alpha_0_s2v/', '../forager/data/lexical_data/50_dim_lexical_data/alpha_0.1_s2v/', '../forager/data/lexical_data/50_dim_lexical_data/alpha_0.2_s2v/', '../forager/data/lexical_data/50_dim_lexical_data/alpha_0.3_s2v/', '../forager/data/lexical_data/50_dim_lexical_data/alpha_0.4_s2v/', '../forager/data/lexical_data/50_dim_lexical_data/alpha_0.5_s2v/', '../forager/data/lexical_data/50_dim_lexical_data/alpha_0.6_s2v/', '../forager/data/lexical_data/50_dim_lexical_data/alpha_0.7_s2v/', '../forager/data/lexical_data/50_dim_lexical_data/alpha_0.8_s2v/', '../forager/data/lexical_data/50_dim_lexical_data/alpha_0.9_s2v/', '../forager/data/lexical_data/50_dim_lexical_data/alpha_1.0_s2v/', '../forager/data/lexical_data/50_dim_lexical_data/alpha_0_w2v/', '../forager/data/lexical_data/50_dim_lexical_data/alpha_0.1_w2v/', '../forager/data/lexical_data/50_dim_lexical_data/alpha_0.2_w2v/', '../forager/data/lexical_data/50_dim_lexical_data/alpha_0.3_w2v/', '../forager/data/lexical_data/50_dim_lexical_data/alpha_0.4_w2v/', '../forager/data/lexical_data/50_dim_lexical_data/alpha_0.5_w2v/', '../forager/data/lexical_data/50_dim_lexical_data/alpha_0.6_w2v/', '../forager/data/lexical_data/50_dim_lexical_data/alpha_0.7_w2v/', '../forager/data/lexical_data/50_dim_lexical_data/alpha_0.8_w2v/', '../forager/data/lexical_data/50_dim_lexical_data/alpha_0.9_w2v/', '../forager/data/lexical_data/50_dim_lexical_data/alpha_1.0_w2v/', '../forager/data/lexical_data/100_dim_lexical_data/alpha_0_s2v/', '../forager/data/lexical_data/100_dim_lexical_data/alpha_0.1_s2v/', '../forager/data/lexical_data/100_dim_lexical_data/alpha_0.2_s2v/', '../forager/data/lexical_data/100_dim_lexical_data/alpha_0.3_s2v/', '../forager/data/lexical_data/100_dim_lexical_data/alpha_0.4_s2v/', '../forager/data/lexical_data/100_dim_lexical_data/alpha_0.5_s2v/', '../forager/data/lexical_data/100_dim_lexical_data/alpha_0.6_s2v/', '../forager/data/lexical_data/100_dim_lexical_data/alpha_0.7_s2v/', '../forager/data/lexical_data/100_dim_lexical_data/alpha_0.8_s2v/', '../forager/data/lexical_data/100_dim_lexical_data/alpha_0.9_s2v/', '../forager/data/lexical_data/100_dim_lexical_data/alpha_1.0_s2v/', '../forager/data/lexical_data/100_dim_lexical_data/alpha_0_w2v/', '../forager/data/lexical_data/100_dim_lexical_data/alpha_0.1_w2v/', '../forager/data/lexical_data/100_dim_lexical_data/alpha_0.2_w2v/', '../forager/data/lexical_data/100_dim_lexical_data/alpha_0.3_w2v/', '../forager/data/lexical_data/100_dim_lexical_data/alpha_0.4_w2v/', '../forager/data/lexical_data/100_dim_lexical_data/alpha_0.5_w2v/', '../forager/data/lexical_data/100_dim_lexical_data/alpha_0.6_w2v/', '../forager/data/lexical_data/100_dim_lexical_data/alpha_0.7_w2v/', '../forager/data/lexical_data/100_dim_lexical_data/alpha_0.8_w2v/', '../forager/data/lexical_data/100_dim_lexical_data/alpha_0.9_w2v/', '../forager/data/lexical_data/100_dim_lexical_data/alpha_1.0_w2v/', '../forager/data/lexical_data/200_dim_lexical_data/alpha_0_s2v/', '../forager/data/lexical_data/200_dim_lexical_data/alpha_0.1_s2v/', '../forager/data/lexical_data/200_dim_lexical_data/alpha_0.2_s2v/', '../forager/data/lexical_data/200_dim_lexical_data/alpha_0.3_s2v/', '../forager/data/lexical_data/200_dim_lexical_data/alpha_0.4_s2v/', '../forager/data/lexical_data/200_dim_lexical_data/alpha_0.5_s2v/', '../forager/data/lexical_data/200_dim_lexical_data/alpha_0.6_s2v/', '../forager/data/lexical_data/200_dim_lexical_data/alpha_0.7_s2v/', '../forager/data/lexical_data/200_dim_lexical_data/alpha_0.8_s2v/', '../forager/data/lexical_data/200_dim_lexical_data/alpha_0.9_s2v/', '../forager/data/lexical_data/200_dim_lexical_data/alpha_1.0_s2v/', '../forager/data/lexical_data/200_dim_lexical_data/alpha_0_w2v/', '../forager/data/lexical_data/200_dim_lexical_data/alpha_0.1_w2v/', '../forager/data/lexical_data/200_dim_lexical_data/alpha_0.2_w2v/', '../forager/data/lexical_data/200_dim_lexical_data/alpha_0.3_w2v/', '../forager/data/lexical_data/200_dim_lexical_data/alpha_0.4_w2v/', '../forager/data/lexical_data/200_dim_lexical_data/alpha_0.5_w2v/', '../forager/data/lexical_data/200_dim_lexical_data/alpha_0.6_w2v/', '../forager/data/lexical_data/200_dim_lexical_data/alpha_0.7_w2v/', '../forager/data/lexical_data/200_dim_lexical_data/alpha_0.8_w2v/', '../forager/data/lexical_data/200_dim_lexical_data/alpha_0.9_w2v/', '../forager/data/lexical_data/200_dim_lexical_data/alpha_1.0_w2v/', '../forager/data/lexical_data/300_dim_lexical_data/alpha_0_s2v/', '../forager/data/lexical_data/300_dim_lexical_data/alpha_0.1_s2v/', '../forager/data/lexical_data/300_dim_lexical_data/alpha_0.2_s2v/', '../forager/data/lexical_data/300_dim_lexical_data/alpha_0.3_s2v/', '../forager/data/lexical_data/300_dim_lexical_data/alpha_0.4_s2v/', '../forager/data/lexical_data/300_dim_lexical_data/alpha_0.5_s2v/', '../forager/data/lexical_data/300_dim_lexical_data/alpha_0.6_s2v/', '../forager/data/lexical_data/300_dim_lexical_data/alpha_0.7_s2v/', '../forager/data/lexical_data/300_dim_lexical_data/alpha_0.8_s2v/', '../forager/data/lexical_data/300_dim_lexical_data/alpha_0.9_s2v/', '../forager/data/lexical_data/300_dim_lexical_data/alpha_1.0_s2v/', '../forager/data/lexical_data/300_dim_lexical_data/alpha_0_w2v/', '../forager/data/lexical_data/300_dim_lexical_data/alpha_0.1_w2v/', '../forager/data/lexical_data/300_dim_lexical_data/alpha_0.2_w2v/', '../forager/data/lexical_data/300_dim_lexical_data/alpha_0.3_w2v/', '../forager/data/lexical_data/300_dim_lexical_data/alpha_0.4_w2v/', '../forager/data/lexical_data/300_dim_lexical_data/alpha_0.5_w2v/', '../forager/data/lexical_data/300_dim_lexical_data/alpha_0.6_w2v/', '../forager/data/lexical_data/300_dim_lexical_data/alpha_0.7_w2v/', '../forager/data/lexical_data/300_dim_lexical_data/alpha_0.8_w2v/', '../forager/data/lexical_data/300_dim_lexical_data/alpha_0.9_w2v/', '../forager/data/lexical_data/300_dim_lexical_data/alpha_1.0_w2v/']\n",
      "\n",
      "average path num: 4\n",
      "['../forager/data/lexical_data/50_dim_lexical_data/average/', '../forager/data/lexical_data/100_dim_lexical_data/average/', '../forager/data/lexical_data/200_dim_lexical_data/average/', '../forager/data/lexical_data/300_dim_lexical_data/average/']\n"
     ]
    }
   ],
   "source": [
    "dimensions = [\"50\", '100', '200', '300']\n",
    "alphas = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "models = [\"s2v\", \"w2v\"]\n",
    "\n",
    "\n",
    "# paths \n",
    "lexical_data_path = \"../forager/data/lexical_data/\"\n",
    "s2v_path = \"_dim_lexical_data/only_s2v/embeddings.csv\"\n",
    "w2v_path = \"_dim_lexical_data/only_w2v/embeddings.csv\"\n",
    "folder = \"_dim_lexical_data/alpha_\"\n",
    "embeddings = \"/embeddings.csv\"\n",
    "\n",
    "count = 0 \n",
    "for dim in dimensions: \n",
    "    for model in models: \n",
    "        for alp in alphas: \n",
    "            path = lexical_data_path + dim + folder + str(alp) + \"_\" + model + \"/\"\n",
    "            alpha_path += [path] \n",
    "            \n",
    "print(\"alpha path num:\", len(alpha_path))\n",
    "print(alpha_path)\n",
    "print()\n",
    "\n",
    "\n",
    "for dim in dimensions: \n",
    "    ave_path += [lexical_data_path + dim + \"_dim_lexical_data/average/\"]\n",
    "\n",
    "print(\"average path num:\", len(ave_path))\n",
    "print(ave_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy Phonological Matrix and Frequency into Alpha/Average directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n",
      "92\n"
     ]
    }
   ],
   "source": [
    "total_path = ave_path + alpha_path\n",
    "print(len(total_path))\n",
    "\n",
    "frequency_file = \"../forager/data/lexical_data/50_dim_lexical_data/only_w2v/frequencies.csv\"\n",
    "phon_matrix_file = \"../forager/data/lexical_data/50_dim_lexical_data/only_w2v/phon_matrix.csv\"\n",
    "\n",
    "counter = 0 \n",
    "for path in total_path: \n",
    "    shutil.copy(frequency_file, path)\n",
    "    shutil.copy(phon_matrix_file, path)\n",
    "    counter += 1\n",
    "\n",
    "print(counter)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Matrix Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from functools import lru_cache\n",
    "from itertools import product as iterprod\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_semantic_matrix(path_to_embeddings, path_for_lexical_data=None):\n",
    "    '''\n",
    "        Description:\n",
    "            Takes in N word embeddings and returns a semantic similarity matrix (NxN np.array)\n",
    "        Args:\n",
    "            (1) path_to_embeddings (str): path to a .csv file containing N word embeddings of size D each (DxN array)\n",
    "        Returns: \n",
    "            (1) semantic_matrix: semantic similarity matrix (NxN np.array)\n",
    "    '''\n",
    "    embeddings = pd.read_csv(path_to_embeddings, encoding=\"unicode-escape\").transpose().values\n",
    "    N = len(embeddings)\n",
    "    \n",
    "    semantic_matrix = 1-scipy.spatial.distance.cdist(embeddings, embeddings, 'cosine').reshape(-1)\n",
    "    semantic_matrix = semantic_matrix.reshape((N,N))\n",
    "    # convert to dataframe without header or index\n",
    "    semantic_matrix_df = pd.DataFrame(semantic_matrix)\n",
    "    semantic_matrix_df.to_csv(path_for_lexical_data + 'semantic_matrix.csv', header=False, index=False)\n",
    "    \n",
    "    '''\n",
    "    #changed\n",
    "    # semantic_matrix_df.to_csv(\"data processing/Lexical Data/word2vec/semantic_matrix.csv\", header=False, index=False)\n",
    "    '''\n",
    "    return semantic_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "counter = 0 \n",
    "\n",
    "for path in total_path: \n",
    "    create_semantic_matrix(path + 'embeddings.csv', path)\n",
    "    counter += 1\n",
    "\n",
    "print(counter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
